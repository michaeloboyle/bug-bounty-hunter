name: Performance Tests

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
        type: string
      
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '10'
        type: string

jobs:
  performance-test:
    name: Performance & Load Testing
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start system
        run: |
          docker compose up -d
          sleep 30

      - name: Install load testing tools
        run: |
          pip install locust requests

      - name: Create Locust test file
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import json
          
          class BugBountyUser(HttpUser):
              wait_time = between(1, 3)
              host = "http://localhost:8080"
              
              @task(3)
              def get_programs(self):
                  self.client.get("/programs")
              
              @task(2)
              def get_findings(self):
                  self.client.get("/findings")
              
              @task(2)
              def get_activities(self):
                  self.client.get("/activities")
              
              @task(1)
              def get_mcp_status(self):
                  self.client.get("/mcp/status")
              
              @task(1)
              def get_analytics(self):
                  self.client.get("/analytics/revenue")
                  self.client.get("/analytics/vulnerabilities")
              
              @task(1)
              def queue_scan(self):
                  # Only queue scans occasionally to avoid overwhelming
                  self.client.post("/queue", json={
                      "program_id": "test-program",
                      "priority": "fast_pay"
                  })
          EOF

      - name: Run load test
        run: |
          DURATION=${{ github.event.inputs.duration || '300' }}
          USERS=${{ github.event.inputs.concurrent_users || '10' }}
          
          locust -f locustfile.py --headless \
            --users $USERS \
            --spawn-rate 2 \
            --run-time ${DURATION}s \
            --html performance-report.html \
            --csv performance-data

      - name: Test database performance
        run: |
          # Test database connection pooling and query performance
          docker compose exec -T db psql -U postgres -d bbops -c "
            SELECT 
              schemaname,
              tablename,
              attname,
              n_distinct,
              correlation
            FROM pg_stats 
            WHERE schemaname = 'public'
            ORDER BY tablename, attname;
          " || echo "No data yet - testing with empty DB"

      - name: Test Redis performance
        run: |
          # Test Redis performance
          docker compose exec -T redis redis-benchmark -q -t ping,set,get -c 10 -n 1000

      - name: Check API response times
        run: |
          # Test individual endpoint response times
          echo "Testing API response times:"
          
          for endpoint in "/programs" "/findings" "/activities" "/mcp/status"; do
            echo -n "Testing $endpoint: "
            time curl -s http://localhost:8080$endpoint > /dev/null
          done

      - name: Memory and CPU usage check
        run: |
          echo "Container resource usage:"
          docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}\t{{.BlockIO}}"

      - name: Generate performance summary
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Parse Locust results
          if [ -f "performance-data_stats.csv" ]; then
            echo "### Load Test Summary" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            head -10 performance-data_stats.csv >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Container stats
          echo "### Resource Usage" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-results
          path: |
            performance-report.html
            performance-data*.csv
            
      - name: Check performance thresholds
        run: |
          # Check if performance meets requirements
          if [ -f "performance-data_stats.csv" ]; then
            # Extract average response time for critical endpoints
            AVG_RESPONSE=$(awk -F',' 'NR>1 {sum+=$6; count++} END {print sum/count}' performance-data_stats.csv)
            
            # Fail if average response time > 2 seconds
            if (( $(echo "$AVG_RESPONSE > 2000" | bc -l) )); then
              echo "❌ Performance test failed: Average response time ${AVG_RESPONSE}ms exceeds 2000ms threshold"
              exit 1
            else
              echo "✅ Performance test passed: Average response time ${AVG_RESPONSE}ms"
            fi
          fi

      - name: Stop system
        if: always()
        run: docker compose down -v

  # Stress test for high-load scenarios
  stress-test:
    name: Stress Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start system with resource limits
        run: |
          # Start with limited resources to test under pressure
          docker compose up -d
          sleep 30

      - name: Install stress testing tools
        run: |
          sudo apt-get update
          sudo apt-get install -y apache2-utils wrk

      - name: Apache Bench stress test
        run: |
          echo "Running Apache Bench stress test..."
          ab -n 10000 -c 100 http://localhost:8080/programs || echo "Stress test completed with some failures (expected)"

      - name: wrk stress test
        run: |
          echo "Running wrk stress test..."
          wrk -t4 -c100 -d30s --latency http://localhost:8080/programs || echo "Stress test completed"

      - name: Memory pressure test
        run: |
          echo "Testing under memory pressure..."
          # Simulate high memory usage
          docker run --rm -d --name memory-hog alpine sh -c 'dd if=/dev/zero of=/tmp/bigfile bs=1M count=500; sleep 60'
          
          # Test API under pressure
          curl -f http://localhost:8080/programs
          curl -f http://localhost:8080/mcp/status
          
          docker stop memory-hog || true

      - name: CPU pressure test
        run: |
          echo "Testing under CPU pressure..."
          # Simulate high CPU usage
          docker run --rm -d --name cpu-hog alpine sh -c 'yes > /dev/null & yes > /dev/null & yes > /dev/null & yes > /dev/null & sleep 60'
          
          # Test API under pressure
          curl -f http://localhost:8080/programs
          curl -f http://localhost:8080/mcp/status
          
          docker stop cpu-hog || true

      - name: Check system recovery
        run: |
          echo "Checking system recovery after stress..."
          sleep 10
          
          # Verify all services are still responsive
          curl -f http://localhost:8080/docs
          curl -f http://localhost:4173/
          
          echo "System recovery successful ✅"

      - name: Stop system
        if: always()
        run: docker compose down -v